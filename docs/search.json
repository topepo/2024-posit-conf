[
  {
    "objectID": "index.html#probability-predictions",
    "href": "index.html#probability-predictions",
    "title": "Evaluating Time-to-Event Models is Hard",
    "section": "Probability Predictions",
    "text": "Probability Predictions"
  },
  {
    "objectID": "index.html#compute-metrics-at-specific-times",
    "href": "index.html#compute-metrics-at-specific-times",
    "title": "Evaluating Time-to-Event Models is Hard",
    "section": "Compute Metrics at Specific Times",
    "text": "Compute Metrics at Specific Times"
  },
  {
    "objectID": "index.html#classificationish-metrics",
    "href": "index.html#classificationish-metrics",
    "title": "Evaluating Time-to-Event Models is Hard",
    "section": "Classification(ish) Metrics",
    "text": "Classification(ish) Metrics\nMost dynamic metrics convert the observed event time to a binary event/non-event representation (at a specific evaluation time).\nFrom there, we can apply existing classification metrics, such as\n\nBrier Score (for calibration)\nArea under the ROC curve (for separation)\n\nWe’ll talk about both of these.\nThere are more details on dynamics metrics at tidymodels.org."
  },
  {
    "objectID": "index.html#converting-to-events",
    "href": "index.html#converting-to-events",
    "title": "Evaluating Time-to-Event Models is Hard",
    "section": "Converting to Events",
    "text": "Converting to Events\nFor a specific evaluation time point \\(\\tau\\), we convert the observed event time to a binary event/non-event version (if possible) (\\(y_{i\\tau} \\in \\{0, 1\\}\\)).\n\\[\ny_{i\\tau} =\n\\begin{cases}\n1 & \\text{if } t_{i} \\leq \\tau\\text{ and  event} \\notag \\\\\n0 & \\text{if } t_{i} \\gt \\tau \\text{ and } either \\notag \\\\\nmissing & \\text{if } t_{i} \\leq \\tau\\text{ and censored }\n\\end{cases}\n\\]"
  },
  {
    "objectID": "index.html#converting-to-events-1",
    "href": "index.html#converting-to-events-1",
    "title": "Evaluating Time-to-Event Models is Hard",
    "section": "Converting to Events",
    "text": "Converting to Events"
  },
  {
    "objectID": "index.html#dealing-with-missing-outcome-data",
    "href": "index.html#dealing-with-missing-outcome-data",
    "title": "Evaluating Time-to-Event Models is Hard",
    "section": "Dealing with Missing Outcome Data",
    "text": "Dealing with Missing Outcome Data\nWithout censored data points, this conversion would yield appropriate performance estimates since no event outcomes would be missing.\n\nOtherwise, there is the potential for bias due to missingness.\n\n\nWe’ll use tools from causal inference to compensate by creating a propensity score that uses the probability of being censored/missing."
  },
  {
    "objectID": "index.html#censoring-weights",
    "href": "index.html#censoring-weights",
    "title": "Evaluating Time-to-Event Models is Hard",
    "section": "Censoring Weights",
    "text": "Censoring Weights\nWe currently use a simple, non-informative censoring model called a “reverse Kaplan-Meier” curve.\n\nFor a given evaluation time, we can compute the probability of any sampleing being censored at \\(\\tau\\).\n\nOur metrics use case weights use the inverse of this probability. See Graf et al (1999)."
  },
  {
    "objectID": "index.html#sum-of-weights-over-time",
    "href": "index.html#sum-of-weights-over-time",
    "title": "Evaluating Time-to-Event Models is Hard",
    "section": "Sum of Weights Over Time",
    "text": "Sum of Weights Over Time"
  },
  {
    "objectID": "index.html#brier-score",
    "href": "index.html#brier-score",
    "title": "Evaluating Time-to-Event Models is Hard",
    "section": "Brier Score",
    "text": "Brier Score\nThe Brier score is calibration metric originally meant for classification models:\n\\[\nBrier = \\frac{1}{N}\\sum_{i=1}^N\\sum_{k=1}^C (y_{ik} - \\hat{\\pi}_{ik})^2\n\\]\nFor our application, we have two classes and case weights\n\\[\nBrier_{\\tau} = \\frac{1}{W_\\tau}\\sum_{i=1}^N\\sum_{k=1}^C w_{it}(y_{ik\\tau} - \\hat{\\pi}_{ik\\tau})^2\n\\]"
  },
  {
    "objectID": "index.html#brier-score-code",
    "href": "index.html#brier-score-code",
    "title": "Evaluating Time-to-Event Models is Hard",
    "section": "Brier Score Code",
    "text": "Brier Score Code\n\n# Out-of-sample predictions at many time points\n# Results contain survival probabilities and case weights in a \n# list column called `.pred`\nval_pred &lt;- augment(mod_fit, new_data = churn_val, eval_time = 5:230)\n\nval_brier &lt;- brier_survival(val_pred, truth = event_time, .pred)\nval_brier %&gt;% filter(.eval_time %in% seq(30, 210, by = 30))\n#&gt; # A tibble: 7 × 4\n#&gt;   .metric        .estimator .eval_time .estimate\n#&gt;   &lt;chr&gt;          &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 brier_survival standard           30   0.00255\n#&gt; 2 brier_survival standard           60   0.00879\n#&gt; 3 brier_survival standard           90   0.0243 \n#&gt; 4 brier_survival standard          120   0.0690 \n#&gt; 5 brier_survival standard          150   0.0977 \n#&gt; 6 brier_survival standard          180   0.153  \n#&gt; 7 brier_survival standard          210   0.188"
  },
  {
    "objectID": "index.html#integrated-brier-score",
    "href": "index.html#integrated-brier-score",
    "title": "Evaluating Time-to-Event Models is Hard",
    "section": "Integrated Brier Score",
    "text": "Integrated Brier Score\n\nbrier_survival_integrated(val_pred, truth = event_time, .pred)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric                   .estimator .estimate\n#&gt;   &lt;chr&gt;                     &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 brier_survival_integrated standard      0.0783"
  },
  {
    "objectID": "index.html#brier-scores-over-evaluation-time",
    "href": "index.html#brier-scores-over-evaluation-time",
    "title": "Evaluating Time-to-Event Models is Hard",
    "section": "Brier Scores Over Evaluation Time",
    "text": "Brier Scores Over Evaluation Time"
  },
  {
    "objectID": "index.html#calibration-over-time",
    "href": "index.html#calibration-over-time",
    "title": "Evaluating Time-to-Event Models is Hard",
    "section": "Calibration Over Time",
    "text": "Calibration Over Time"
  },
  {
    "objectID": "index.html#area-under-the-roc-curve",
    "href": "index.html#area-under-the-roc-curve",
    "title": "Evaluating Time-to-Event Models is Hard",
    "section": "Area Under the ROC Curve",
    "text": "Area Under the ROC Curve\nThis is more straightforward.\n\nWe can use the standard ROC curve machinery once we have the indicators, probabilities, and censoring weights at evaluation time \\(\\tau\\) (Hung and Chiang (2010)).\n\nROC curves measure the separation between events and non-events and are ignorant of how well-calibrated the probabilities are."
  },
  {
    "objectID": "index.html#area-under-the-roc-curve-1",
    "href": "index.html#area-under-the-roc-curve-1",
    "title": "Evaluating Time-to-Event Models is Hard",
    "section": "Area Under the ROC Curve",
    "text": "Area Under the ROC Curve\n\nval_roc_auc &lt;- roc_auc_survival(val_pred, truth = event_time, .pred)\nval_roc_auc %&gt;% filter(.eval_time %in% seq(30, 210, by = 30))\n#&gt; # A tibble: 7 × 4\n#&gt;   .metric          .estimator .eval_time .estimate\n#&gt;   &lt;chr&gt;            &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 roc_auc_survival standard           30     0.999\n#&gt; 2 roc_auc_survival standard           60     0.999\n#&gt; 3 roc_auc_survival standard           90     0.984\n#&gt; 4 roc_auc_survival standard          120     0.961\n#&gt; 5 roc_auc_survival standard          150     0.928\n#&gt; 6 roc_auc_survival standard          180     0.835\n#&gt; 7 roc_auc_survival standard          210     0.859"
  },
  {
    "objectID": "index.html#roc-auc-over-evaluation-time",
    "href": "index.html#roc-auc-over-evaluation-time",
    "title": "Evaluating Time-to-Event Models is Hard",
    "section": "ROC AUC Over Evaluation Time",
    "text": "ROC AUC Over Evaluation Time"
  },
  {
    "objectID": "index.html#conculsion",
    "href": "index.html#conculsion",
    "title": "Evaluating Time-to-Event Models is Hard",
    "section": "Conculsion",
    "text": "Conculsion\n\nBad news: statistically, this is pretty convoluted.\nGood news: tidymodels handles the details and provides a clean syntax for some complex statistics.\n\n\nA huge thanks to the tidymodels team: Hannah Frick, Emil Hvitfeldt, and Simon Couch!"
  },
  {
    "objectID": "index.html#reverse-kaplan-meier-curve",
    "href": "index.html#reverse-kaplan-meier-curve",
    "title": "Evaluating Time-to-Event Models is Hard",
    "section": "“Reverse Kaplan-Meier” Curve",
    "text": "“Reverse Kaplan-Meier” Curve\n\n\nWe assume a non-informative censoring model: to compute the probability\n\n\n\n\n\n\n\n\n\n\nFor each prediction at evaluation time \\(\\tau\\), we compute the probability at an adjusted time:\n\\[\nt_i^*=\n\\begin{cases}\nt_i  - \\epsilon &  \\text{if }t_i \\le \\tau \\\\ \\notag\n\\tau - \\epsilon &  \\text{if }t_i &gt; \\tau  \\notag\n\\end{cases}\n\\]\n\n\n\n\nhttps://topepo.github.io/2024-posit"
  }
]